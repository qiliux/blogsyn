---
title: 二分类LG理解
date: 2017-07-29 16:48:07
tags:
- 机器学习
- math
categories:
- 机器学习
---

LG:Logistic Regression  
=============
Logistic 函数：  
-------------
　　s型函数（sigmod函数），单调函数，处处可微，$1-h(x) = h(-x)$  
　　$\theta(s)=\frac{e^s}{e^s+1}\\\\
　　=\frac{1}{1+e^{-s}} （y值缩放）$  
　　LR的假设空间，  
　　　　$ h(x)=\frac{1}{1+e^{-s}}\\\\
		 y=1　if　h(x) >= 0.5\\\\
		 y=0　if　h(x) <  0.5 $  
　　如果我们能将一些特征映射到Logistic函数，然后进行函数值的范围来进行分类，这样会不会有效果呢？  
　　则对于二分类任务：  
　　　　输入：{% math %}x_{i}=(x_{i1},......,x_{id}){% endmath %}  
　　　　输出：$y\in{0,1}$  
　　首先，我们将输入加权得到一个分数：{% math %}s=\sum_{i=0}^{d}w_ix_i{% endmath %}  
　　然后，通过logistic函数进行映射？能直接进行映射吗？留下一个疑问，最后的值域是$[0,1]$  
　　最后与阈值$0.5$比较，得到预测的分类  
　　好啦，然后我们来点难的数学推理吧。这里推理随机梯度下降SGD，我们的目标是求出$w$向量。  
　　首先我们定义每一个实例属于其分类值的概率如下：  
　　{%math%}P(y=1|x;w)=h_w(x),P(y=0|x;w)=1-h_w(x){%endmath%}  
　　{%math%}P(y|x;w)=(h_w(x))^y(h_w(x))^{1-y}{%endmath%}  
　　然后我们统计每个实例都属于其分类的概率,下面求解的方法又称极大似然估计，即令每个样本属于其真实值得概率越大越好：  
　　{%math%}L(w)=P(y|X;w)=\prod_{i=1}^{m}((h_w(x_i))^{yi}(1-h_w(x_i))^{1-yi}){%endmath%}  
　　由于连乘不好处理，我们对其求$log$变为连加，取$log$会不会改变其影响求极小值点对应的$w$呢？既然这里用了，肯定不影响，原因待补。  
　　{%math%}l(w)=log(L(w))=\sum_{i=1}^{m}(y_ilog(h_w(x_i)^{yi})+(1-y_i)log(1-h_w(x_i))){%endmath%}  
