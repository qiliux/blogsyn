---
title: 文本特征处理
date: 2017-04-28 13:12:48
categories:
- nlp
- 自然语言处理
tags:
- BOW
- Word2Vec
---

下面介绍的模型都是将文本数据转化为向量数据，方便下一步的处理。

【先前工作 Important】
================
- 分词处理
- 文档是一个文本的基本单元
- 文档集由若干文档组成

BOW-词袋模型- Bag of Words
=================
概述：将一个文档集的所有词都归入一个集合 *wordset* 中。*wordset*的大小是我们目标向量的长度，向量中的每一位由当前位置wordset的词在本文档出现次数决定。  
{% note info %}  
下面举个例子：  
　　数据如下：（都是分词后的结果）  
　　　　文档1： 学校 是 个 好 地方  
　　　　文档2： 我 不 喜欢 去 学校   
　　　　*wordset*：{学校 是 个 好 地方 我 不 喜欢 去} 长度为9，则目标向量长度为9，转换后的目标向量如下：  
　　　　向量1：[1,1,1,1,1,0,0,0,0]  
　　　　向量2：[1,0,0,0,0,1,1,1,1]  
{% endnote %}
 好啦，希望你明白了BOW的表示形式。BOW不考虑词与词之间的顺序，假设词与词之间是孤立的，最后得到的特征是稀疏的。  
 实际使用时对与较大的文本，请先进行降维（如PCA），然后再使用对应的算法。原因是：较大的文件会产生较大的*wordset*，那会造成每一个文档用多少维向量表示呢？相关请查看 [维数灾难](https://zh.wikipedia.org/wiki/%E7%BB%B4%E6%95%B0%E7%81%BE%E9%9A%BE)。

TF-IDF
=================

概述：根据文档和文档集对每个词都赋予一定的权重，用来表示该文档的重要性，重要性通过两个方面来度量：  
　　TF：词频-文档中出现词W的次数，次数越高的词越重要  
　　　　$tf\_{word} = \frac{word在文档中出现的次数}{文档中词的总次数}$  
　　IDF：逆文档频率-词W在多少个文档中出现过，衡量词的常见程度，越常见的词信息量越低  
　　　　$idf\_{word} = log(\frac{文档集中文档总数}{包含word的文档数+1})$  
最终：$ TF-IDF=TF*IDF $  
{% note info %}
下面我们在举个例子：  
　　文档1： 学校 是 个 好 地方  
　　文档2： 我 不 喜欢 去 学校   
　　文档3： 学校 的 花 开 了   
　　我们来计算学校这个词的TF-IDF值  
　　$tf\_{学校}=1/5=0.2$  
　　$idf\_{学校}=log(3/(3+1))=-0.287$
　　$TF-IDF\_{学校}=tf\_{学校}\*idf\_{学校} = 0.2*-0.287=-0.057$ 思考，为负数意味着什么？这个词大家都有，在分类和回归中用处基本没有什么用  
{% endnote %}
好啦，一个词的TF-IDF值我们计算好了,那一个文档的向量对应的是什么呢,那么也像词袋模型一样，根据词典的词的数量来确定向量的大小吧。  
{% note info %}
　　词典 {   学校   是    个     好    地方  我  不  喜欢 去  的 花   开 了}  
　　文档1：  学校   是    个     好    地方  
　　向量1：[-0.057,0.081,0.081,0.081,0.138  ,0  ,0  ,0   ,0 ,0  ,0 ,0  ,0 ]  
　　文档2： 我 不 喜欢 去 学校  
　　　　　......   
　　文档3： 学校 的 花 开 了   
　　　　  .....  
{% endnote %}  

Word2Vec模型  
=================


h哈哈哈  
哈哈哈哈